{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, time, datetime, random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.5)\n",
       "  (fc1): Linear(in_features=4096, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=227, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# model = m.ConvNet()\n",
    "model = torch.load(\"conv_network_model\", map_location=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image(img, img_size):\n",
    "    # scale and pad image\n",
    "    \n",
    "    img_transforms= transforms.Compose(\n",
    "        [transforms.Resize(img_size),     \n",
    "         transforms.ToTensor(),\n",
    "         ])\n",
    "    # convert image to Tensor\n",
    "    \n",
    "    tensorimage = img_transforms(img).float()\n",
    "    image_tensor = tensorimage.view(1,1,size_IMG,size_IMG)\n",
    "#     input_img = Variable(image_tensor)\n",
    "    # run inference on the model and get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(image_tensor)\n",
    "        detections = utils.non_max_suppression(detections, 80, \n",
    "                        conf_thres, nms_thres)\n",
    "    return detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load image and get detections\n",
    "# img_path = \"./data/ä¸š/Accompany you forever Font-Simplified Chinesettf.png\"\n",
    "# prev_time = time.time()\n",
    "# img = Image.open(img_path)\n",
    "# print(img.size)\n",
    "# detections = detect_image(img,img_size)\n",
    "\n",
    "\n",
    "# inference_time = datetime.timedelta(seconds=time.time() - prev_time)\n",
    "# print ('Inference Time: %s' % (inference_time))\n",
    "# # Get bounding-box colors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "img = np.array(img)\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(1, figsize=(12,9))\n",
    "ax.imshow(img)\n",
    "pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "unpad_h = img_size - pad_y\n",
    "unpad_w = img_size - pad_x\n",
    "\n",
    "\n",
    "if detections is not None:\n",
    "    unique_labels = detections[:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    # browse detections and draw bounding boxes\n",
    "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "        box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "        box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "        y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "        x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "        color = bbox_colors[int(np.where(\n",
    "             unique_labels == int(cls_pred))[0])]\n",
    "        bbox = patches.Rectangle((x1, y1), box_w, box_h,\n",
    "             linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(bbox)\n",
    "        plt.text(x1, y1, s=classes[int(cls_pred)], \n",
    "                color='white', verticalalignment='top',\n",
    "                bbox={'color': color, 'pad': 0})\n",
    "plt.axis('off')\n",
    "# save image\n",
    "plt.savefig(img_path.replace(\".jpg\", \"-det.jpg\"),        \n",
    "                  bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
